# Testing
## The go test Tool
1. In a package directory, files whose names end with `_test.go` are not part of the package ordinarily built by `go build` but are a part of it when built by `go test`.
2. Within `*_test.go` files, three kinds of functions are treated specially: tests, benchmarks, and examples. A test function, which is a function whose name begins with `Test`, exercises some program logic for correct behavior; `go test` calls the test function and reports the result, which is either PASS or FAIL. A benchmark function has a name beginning with Benchmark and measures the performance of some operation; go test reports the mean execution time of the operation. And an example function, whose name starts with Example, provides machine-checked documentation.
3. The go test tool scans the `*_test.go` files for these special functions, generates a temporary main package that calls them all in the proper way, builds and runs it, reports the results, and then cleans up.

## Test Functions
1. Each test file must import the `testing` package. Test function names must begin with Test; the optional suffix Name must begin with a capital letter. Test functions have the following signature: `func TestName(t *testing.T) { /* ... */ }`. The `t` parameter provides methods for reporting test failures and logging additional information.
2. `go test` supports `-run` flag, whose argument is a regular expression, causes go test to run only those tests whose function name matches the pattern.
3. This style of table-driven testing is very common in Go. It is straightforward to add new table entries as needed, and since the assertion logic is not duplicated, we can invest more effort in producing a good error message.
4. The output of a failing test does not include the entire stack trace at the moment of the call to t.Errorf. Nor does t.Errorf cause a panic or stop the execution of the test, unlike assertion failures in many test frameworks for other languages. Tests are independent of each other. If an early entry in the table causes the test to fail, later table entries will still be checked, and thus we may learn about multiple failures during a single run.
5. When we really must stop a test function, perhaps because some initialization code failed or to prevent a failure already reported from causing a confusing cascade of others, we use t.Fatal or t.Fatalf. These must be called from the same goroutine as the Test function, not from another one created during the test.
6. Test failure messages are usually of the form "f(x) = y, want z", where f(x) explains the attempted operation and its input, y is the actual result, and z the expected result.
7. It’s important that code being tested not call `log.Fatal` or `os.Exit`, since these will stop the process in its tracks; calling these functions should be regarded as the exclusive right of main.
8. A black-box test assumes nothing about the package other than what is exposed by its API and specified by its documentation; the package’s internals are opaque. In contrast, a white-box test has privileged access to the internal functions and data structures of the package and can make observations and changes that an ordinary client cannot. The two approaches are complementary. Black-box tests are usually more robust, needing fewer updates as the software evolves. They also help the test author empathize with the client of the package and can reveal flaws in the API design. In contrast, white-box tests can provide more detailed coverage of the trickier parts of the implementation.
9. The global variables at package level can be mocked to support white-box tests, but make sure to restore the original ones using `defer` after the test. This pattern can be used to temporarily save and restore all kinds of global variables, including command-line flags, debugging options, and performance parameters; to install and remove hooks that cause the production code to call some test code when something interesting happens; and to coax the production code into rare but important states, such as timeouts, errors, and even specific interleavings of concurrent activities. Using global variables in this way is safe only because go test does not normally run multiple tests concurrently.
10. We can declare the test function in an external test package by adding extra suffix `_test` to the package name, which is a signal to `go test` that it should build an additional package containing just these files and run its tests. The external test package lives in a separate package, they may import helper packages that also depend on the package being tested, and is logically higher up than both of the packages it depends upon. By avoiding import cycles, external test packages allow tests, especially integration tests (which test the interaction of several components), to import other packages freely, exactly as an application would.
11. Sometimes an external test package may need privileged access to the internals of the package under test, if for example a white-box test must live in a separate package to avoid an import cycle. In such cases, we use a trick: we add declarations to an in-package `_test.go` file to expose the necessary internals to the external test. This file thus offers the test a "back door" to the package. If the source file exists only for this purpose and contains no tests itself, it is often called `export_test.go`.
12. In this sense, assertion functions suffer from "premature abstraction": by treating the failure of this particular test as a mere difference of two integers, we forfeit the opportunity to provide meaningful context. Only once repetitive patterns emerge in a given test suite is it time to introduce abstractions, which is often not to define a function to replace the entire assertion statement, but to execute it in a loop for different combinations.
13. An application that often fails when it encounters new but valid inputs is called buggy; a test that spuriously fails when a sound change was made to the program is called brittle. The most brittle tests, which fail for almost any change to the production code, good or bad, are sometimes called change detector or status quo tests, and the time spent dealing with them can quickly deplete any benefit they once seemed to provide. The easiest way to avoid brittle tests is to check only the properties you care about. Test your program’s simpler and more stable interfaces in preference to its internal functions. Be selective in your assertions. Don’t check for exact string matches, for example, but look for relevant substrings that will remain unchanged as the program evolves.

## Coverage
1. By its nature, testing is never complete. As the influential computer scientist Edsger Dijkstra put it, ‘‘Testing shows the presence, not the absence of bugs.’’ No quantity of tests can ever prove a package free of bugs. At best, they increase our confidence that the package works well in a wide range of important scenarios.
2. The degree to which a test suite exercises the package under test is called the test’s coverage. Coverage can’t be quantified directly — the dynamics of all but the most trivial programs are beyond precise measurement — but there are heuristics that can help us direct our testing efforts to where they are more likely to be useful. Statement coverage is the simplest and most widely used of these heuristics. The statement coverage of a test suite is the fraction of source statements that are executed at least once during the test.
3. The usage of the coverage tool in Go includes two parts: first is to generate the coverage profile during the `go test` with parameters `go test -coverprofile=c.out`. Then display the profile using `go tool cover -html=c.out`.

## Benchmark Functions
1. In Go, a benchmark function has the `Benchmark` prefix and a `*testing.B` parameter that provides most of the same methods as a `*testing.T`, plus a few extra related to performance measurement. It also exposes an integer field `N`, which specifies the number of times to perform the operation being measured.
2. Unlike tests, by default no benchmarks are run. The argument to the `-bench` flag selects which benchmarks to run. It is a regular expression matching the names of Benchmark functions, with a default value that matches none of them. The `.` pattern causes it to match all benchmarks in the package. `go test -bench=.`
3. Since the benchmark runner initially has no idea how long the operation takes, it makes some initial measurements using small values of N and then extrapolates to a value large enough for a stable timing measurement to be made.
4. The reason the loop is implemented by the benchmark function, and not by the calling code in the test driver, is so that the benchmark function has the opportunity to execute any necessary one-time setup code outside the loop without this adding to the measured time of each iteration.
5. The fastest program is often the one that makes the fewest memory allocations. The `-benchmem` command-line flag will include memory allocation statistics in its report.
6. Use comparative benchmarks to test the asymptotic growth of the running time of the function, or evaluate two different algorithms on the same input data. They typically take the form of a single parameterized function, called from several Benchmark functions with different values.
    
    ```
     func benchmark(b *testing.B, size int) { ... }
     func Benchmark10(b *testing.B)   { benchmark(b, 10) }
     func Benchmark100(b *testing.B)  { benchmark(b, 100) }
     func Benchmark1000(b *testing.B) { benchmark(b, 1000) }
     ```

## Profiling
1.  Every programmer knows Donald Knuth’s aphorism about premature optimization: "Programmers waste enormous amounts of time thinking about, or worrying about, the speed of noncritical parts of their programs, and these attempts at efficiency actually have a strong negative impact when debugging and maintenance are considered. We should forget about small efficiencies, say about 97% of the time: premature optimization is the root of all evil. Yet we should not pass up our opportunities in that critical 3%. A good programmer will not be lulled into complacency by such reasoning, he will be wise to look carefully at the critical code; but only after that code has been identified. It is often a mistake to make a priori judgments about what parts of a program are really critical, since the universal experience of programmers who have been using measurement tools has been that their intuitive guesses fail."
2. When we wish to look carefully at the speed of our programs, the best technique for identifying the critical code is profiling. Profiling is an automated approach to performance measurement based on sampling a number of profile events during execution, then extrapolating from them during a post-processing step; the resulting statistical summary is called a profile.
3. A CPU profile identifies the functions whose execution requires the most CPU time. The currently running thread on each CPU is interrupted periodically by the operating system every few milliseconds, with each interruption recording one profile event before normal execution resumes.
4. A heap profile identifies the statements responsible for allocating the most memory. The profiling library samples calls to the internal memory allocation routines so that on average, one profile event is recorded per 512KB of allocated memory.
5. A blocking profile identifies the operations responsible for blocking goroutines the longest, such as system calls, channel sends and receives, and acquisitions of locks. The profiling library records an event every time a goroutine is blocked by one of these operations.
6. Gathering a profile for code under test is as easy as enabling one of the flags `-cpuprofile=cpu.out`, `-blockprofile=block.out`, `-memprofile=mem.out`. Be careful when using more than one flag at a time, however: the machinery for gathering one kind of profile may skew the results of others.
7. Once we’ve gathered a profile, we need to analyze it using the pprof tool. This is a standard part of the Go distribution, but since it’s not an everyday tool, it’s accessed indirectly using `go tool pprof`. It requires two arguments, the executable that produced the profile and the profile log. Although `go test` usually discards the test executable once the test is complete, when profiling is enabled it saves the executable as foo.test, where foo is the name of the tested package.

## Example Functions
1. The primary purpose of example functions is documentation: a good example can be a more succinct or intuitive way to convey the behavior of a library function than its prose description, especially when used as a reminder or quick reference. Based on the suffix of the Example function, the web-based documentation server godoc associates example functions with the function or package they exemplify.
2. The second purpose is that examples are executable tests run by `go test`. If the example function contains a final `// Output: comment`, the test driver will execute the function and check that what it printed to its standard output matches the text within the comment.
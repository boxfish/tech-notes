# Concurrency with Shared Variables

## Race Conditions
1. A function is concurrency-safe if it continues to work correctly even when called concurrently, that is, from two or more goroutines with no additional synchronization. A type is concurrency-safe if all its accessible methods and operations are concurrency-safe.
2. Exported package-level functions are generally expected to be concurrency-safe. Since package-level variables cannot be confined to a single goroutine, functions that modify them must enforce mutual exclusion.
3. Deadlock: A situation in which two or more processes are unable to proceed because each is waiting for one the others to do something.
4. Livelock: A situation in which two or more processes continuously change their states in response to changes in the other process(es) without doing any useful work.
5. Starvation: A situation in which a runnable process is overlooked indefinitely by the scheduler; although it is able to proceed, it is never chosen.
6. A race condition is a situation in which the program does not give the correct result for some interleavings of the operations of multiple goroutines. A data race occurs whenever two goroutines access the same variable concurrently and at least one of the accesses is a write.
7. The first way to avoid data race is not to write the variable. Data structures that are never modified or are immutable are inherently concur- rency-safe and need no synchronization. 
8. The second way to avoid a data race is to avoid accessing the variable from multiple goroutines. Confine the variables to a single goroutine and use channels to communicate. This is what is meant by the Go mantra "Do not communicate by sharing memory; instead, share memory by communicating."
9. Even when a variable cannot be confined to a single goroutine for its entire lifetime, confinement may still be a solution to the problem of concurrent access. If each stage of the pipeline refrains from accessing the variable after sending it to the next stage, then all accesses to the variable are sequential. In effect, the variable is confined to one stage of the pipeline, then confined to the next, and so on. This discipline is sometimes called serial confinement.
10. The third way to avoid a data race is to allow many goroutines to access the variable, but only one at a time. This approach is known as mutual exclusion.

## Mutual Exclusion: sync.Mutex
1. we can use a channel of capacity 1 to ensure that at most one goroutine accesses a shared variable at a time. A semaphore that counts only to 1 is called a binary semaphore.
2. This pattern of mutual exclusion is so useful that it is supported directly by the Mutex type from the sync package. Its Lock method acquires the token (called a lock) and its Unlock method releases it.
3. The mutex guards the shared variables. By convention, the variables guarded by a mutex are declared immediately after the declaration of the mutex itself. The region of code between Lock and Unlock in which a goroutine is free to read and modify the shared variables is called a critical section. It is essential that the goroutine release the lock once it is finished, on all paths through the function, including error paths. Go’s defer statement comes to the rescue: by deferring a call to Unlock, the critical section implicitly extends to the end of the current function, freeing us from having to remember to insert Unlock calls in one or more places far from the call to Lock.
4. mutex locks are not re-entrant — it’s not possible to lock a mutex that’s already locked—this leads to a deadlock where nothing can proceed, and Withdraw blocks forever.

## Read/Write Mutexes: sync.RWMutex
1. sync.RWMutex provides a multiple readers, single writer lock, that is a special kind of lock that allows read-only operations to proceed in parallel with each other, but write operations to have fully exclusive access. The reader lock won't wait for other reader lock. The writer lock will wait for all reader locks to release. The reader lock will wait for writer lock to release. 

## Memory Synchronization
1. It is tempting to try to understand concurrency as if it corresponds to some interleaving of the statements of each goroutine, but this is not how a modern compiler or CPU works. For example, if two statements in one goroutine refer to different variables, a compiler may conclude that the order of the two statements cannot affect the result, and swap them. If the two goroutines execute on different CPUs, each with its own cache, writes by one goroutine are not visible to the other goroutine’s read until the caches are synchronized with main memory. All these concurrency problems can be avoided by the consistent use of synchronization primitives like channel communications and mutex operations, which cause the processor to flush out and commit all its accumulated writes so that the effects of goroutine execution up to that point are guaranteed to be visible to goroutines running on other processors. 

## Lazy Initialization: sync.Once
1. Conceptually, a Once consists of a mutex and a boolean variable that records whether initialization has taken place. If the boolean is true, return without doing anything. Otherwise, it tries to lock the mutex that guards both the boolean and the client’s data structures. After gaining the lock, it checks the boolean again, if it is still false, it calls the function passed as argument, and then set the boolean flag to true.

## The Race Detector
1. Just add the `-race` flag to `go build`, `go run` or `go test` command. This causes the compiler to build a modified version of your application or test with additional instrumentation that effectively records all accesses to shared variables that occurred during execution, along with the identity of the goroutine that read or wrote the variable. In addition, the modified program records all synchronization events. The race detector studies this stream of events, looking for cases in which one goroutine reads or writes a shared variable that was most recently written by a different goroutine without an intervening synchronization operation. This indicates a concurrent access to the shared variable, and thus a data race. The race detector reports all data races that were actually executed. However, it can only detect race conditions that occur during a run; it cannot prove that none will ever occur.

## Example: Concurrent Non-Blocking Cache
1. Understand how the same functionality can be achieved by using either shared variable in memo4 and monitoring goroutine using channels in memo5.

## Goroutines and Threads
1. Growable Stacks. Each OS thread has a fixed-size block of memory (often as large as 2MB) for its stack, the work area where it saves the local variables of function calls that are in progress or temporarily suspended while another function is called. Changing the fixed size can improve space efficiency and allow more threads to be created, or it can enable more deeply recursive functions, but it cannot do both. n contrast, a goroutine starts life with a small stack, typically 2KB. A goroutine’s stack, like the stack of an OS thread, holds the local variables of active and suspended function calls, but unlike an OS thread, a goroutine’s stack is not fixed; it grows and shrinks as needed. The size limit for a goroutine stack may be as much as 1GB.
2. Goroutine Scheduling. OS threads are scheduled by the OS kernel. Every few milliseconds, a hardware timer interrupts the processor, which causes a kernel function called the scheduler to be invoked. This function suspends the currently executing thread and saves its registers in memory, looks over the list of threads and decides which one should run next, restores that thread’s registers from memory, then resumes the execution of that thread. Because OS threads are scheduled by the kernel, passing control from one thread to another requires a full context switch, that is, saving the state of one user thread to memory, restoring the state of another, and updating the scheduler’s data structures. This operation is slow, due to its poor locality and the number of memory accesses required, and has historically only gotten worse as the number of CPU cycles required to access memory has increased. The Go runtime contains its own scheduler that uses a technique known as m:n scheduling, because it multiplexes (or schedules) m goroutines on n OS threads. The job of the Go scheduler is analogous to that of the kernel scheduler, but it is concerned only with the goroutines of a single Go program.
Unlike the operating system’s thread scheduler, the Go scheduler is not invoked periodically by a hardware timer, but implicitly by certain Go language constructs. For example, when a goroutine calls time.Sleep or blocks in a channel or mutex operation, the scheduler puts it to sleep and runs another goroutine until it is time to wake the first one up. Because it doesn’t need a switch to kernel context, rescheduling a goroutine is much cheaper than rescheduling a thread.
3. The Go scheduler uses a parameter called GOMAXPROCS to determine how many OS threads may be actively executing Go code simultaneously. Its default value is the number of CPUs on the machine, so on a machine with 8 CPUs, the scheduler will schedule Go code on up to 8 OS threads at once. (GOMAXPROCS is the n in m:n scheduling.)
4. Goroutines Have No Identity. In most operating systems and programming languages that support multithreading, the cur- rent thread has a distinct identity that can be easily obtained as an ordinary value, typically an integer or pointer. This makes it easy to build an abstraction called thread-local storage, which is essentially a global map keyed by thread identity, so that each thread can store and retrieve values independent of other threads. Goroutines have no notion of identity that is accessible to the programmer. This is by design, since thread-local storage tends to be abused.


